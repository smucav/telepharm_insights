# ðŸ”¬ TelePharm Insights

**An End-to-End Data Product: From Raw Telegram Data to an Analytical API**

---

## ðŸ“Œ Project Overview

**TelePharm Insights** is a robust data platform designed to extract, transform, and expose insights about **Ethiopian medical businesses** by scraping public Telegram channels.

This project answers critical questions for stakeholders:
- ðŸ§ª **What are the top mentioned medical products and drugs?**
- ðŸ’° **How does price and availability vary by channel?**
- ðŸ–¼ï¸ **Which channels share the most visual content (images)?**
- ðŸ“ˆ **What are the daily & weekly trends in health-related posts?**

---

## âš™ï¸ Tech Stack

| ðŸ”— Layer | ðŸ“š Tools |
|----------|----------------|
| **Orchestration** | [Dagster](https://dagster.io/) |
| **ELT & Data Modeling** | [dbt](https://www.getdbt.com/) |
| **Database** | PostgreSQL |
| **Scraping** | [Telethon](https://docs.telethon.dev/) |
| **Data Enrichment** | [YOLOv8](https://docs.ultralytics.com/) |
| **API** | [FastAPI](https://fastapi.tiangolo.com/) |
| **Containerization** | Docker, Docker Compose |
| **Secrets Management** | python-dotenv |

---

## âœ… Current Progress

| ðŸ—‚ï¸ Task | âœ… Status | ðŸ“Œ Description |
|----------------------|-----------|----------------------------|
| **Task 0** | âœ”ï¸ Completed | ðŸ“ **Project structure**, environment management, Docker setup, secure `.env` secrets. |
| **Task 1** | âœ”ï¸ Completed | Telegram scraping with raw JSON, images, partitioned by date & channel, robust logging. |
| **Task 2** | âœ”ï¸ Completed | loading json file to database, dbt star schema models, staging and marts |
| **Task 3** | âœ”ï¸ Completed | Data enrichment with YOLOv8 |
| **Task 4** | âœ”ï¸ Completed | Exposing insights via FastAPI |
| **Task 5** | â³ Upcoming | Orchestration with Dagster |

---

## ðŸ—ƒï¸ Project Structure

```
telepharm_insights/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Data lake: raw Telegram JSONs
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ marts/
â”œâ”€â”€ dbt/                # dbt transformation project
â”œâ”€â”€ dags/               # Dagster pipeline definitions
â”œâ”€â”€ api/                # FastAPI source code
â”œâ”€â”€ scripts/            # Telegram scraper, YOLO enrichment, loaders
â”œâ”€â”€ .env                # Secrets & credentials (GIT IGNORED!)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
```

---

## ðŸš€ Local Development

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/telepharm_insights.git
cd telepharm_insights
```

### 2ï¸âƒ£ Create your `.env`

```dotenv
TELEGRAM_API_ID=xxxxxx
TELEGRAM_API_HASH=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
POSTGRES_USER=postgres
POSTGRES_PASSWORD=yourpassword
POSTGRES_DB=telepharm_db
POSTGRES_HOST=db
POSTGRES_PORT=5432
```

**Note:** `.env` is **never committed**. Manage secrets safely!

---

### 3ï¸âƒ£ Run with Docker

```bash
docker-compose -f docker/docker-compose.yml up --build
```

This spins up:
- ðŸ˜ **PostgreSQL** database
- âš¡ **FastAPI** app (API scaffolding â€” future tasks)
- ðŸ“¦ Python environment with all dependencies installed

---

## ðŸ“Œ Task 0: Project Setup & Environment

âœ… **Highlights**
- Professional structure for a reproducible, production-grade pipeline.
- Managed all secrets with `.env` + `python-dotenv`.
- Dockerized environment guarantees portability.
- PostgreSQL container for robust ELT development.
- Version control ready with `.gitignore` hygiene.

---

## ðŸ“Œ Task 1 â€” Data Scraping & Collection

**Status:** âœ”ï¸ Completed

**Description:**
- Developed a robust **Telegram Scraper** using **Telethon**.
- Extracted messages from multiple Ethiopian medical channels:
  - `Chemed123`
  - `lobelia4cosmetics`
  - `tikvahpharma`
- Collected **text messages** and **downloaded images** where available.
- Stored **raw JSON data** in a clear, **partitioned Data Lake** structure:

```data/raw/telegram_messages/YYYY-MM-DD/channel_name/channel_name.json```

- Implemented **logging** with:
- Channel name & scrape date.
- Errors & rate limit handling.
- Download status for images.
- Ensured data format is **dbt-ready** for next steps:
- Flat JSON records.
- Clean fields for fact & dimension modeling.


## âœ¨ Upcoming Tasks

âœ… **Next:**
- **Task 2** Data Modeling and Transformation (Transform)
  - Raw JSON will be loaded to PostgreSQL
  - transformed with **dbt** star schema models


# ðŸ› ï¸ Task 2: Data Modeling and Transformation

## âœ… Status
**âœ”ï¸ Completed**

---

## ðŸŽ¯ Deliverables

### ðŸ“¥ JSON to PostgreSQL

- **Script:** `scripts/load_to_postgres.py`
  - Loads JSON files into `raw.telegram_messages` with fields:
    - `message_id`
    - `channel`
    - `scrape_date`
    - `message_date`
    - `sender_id`
    - `text`
    - `has_image`
    - `image_file`
    - `message_length`
  - Adds `message_length` for analytics.

---

### ðŸ“¦ dbt Project Setup

- Uses existing `dbt/telepharm_dbt/` project.
- Configured `profiles.yml` to connect to PostgreSQL (`telegram_medical` database).
- Updated `dbt_project.yml` with:
  - **Staging schema:** `staging` (views)
  - **Marts schema:** `marts` (tables)

---

### ðŸ”„ Staging Models

- `stg_telegram_messages.sql`:
  - Cleans raw data.
  - Casts dates.
  - Ensures non-null text.

---

### ðŸ“Š Data Mart Models (Star Schema)

- **`dim_channels.sql`:**
  - Dimension table:
    - `channel_id`
    - `channel_name`
    - `first_message_date`
    - `last_message_date`
    - `total_messages`

- **`dim_dates.sql`:**
  - Date dimension with:
    - `date_id`
    - `year`
    - `month`
    - `day`
    - `day_of_week`
    - `week_of_year`
    - `is_weekend`

- **`fct_messages.sql`:**
  - Fact table with message details.
  - Links to `dim_channels` and `dim_dates`.

---

### âœ… Testing

- **Built-in dbt tests:**
  - `unique`, `not_null` for primary keys.
  - `relationships` for foreign keys.
- **Custom test:**
  - `custom_message_length.sql` validates message length consistency.

---

### ðŸ“š Documentation

- Generated dbt documentation:
  - `dbt docs generate`
  - `dbt docs serve`

---

## ðŸš€ Execution Instructions

**Load JSON to PostgreSQL:**
```bash
python scripts/load_to_postgres.py
```
**Verify data in database:**
```bash
make dbt-debug
```
**Run dbt Models:**
```bash
make dbt-run
```
---

## ðŸ› ï¸ Task 3: Data Enrichment with YOLOv8

**Status:** âœ”ï¸ *Completed*

---

### ðŸŽ¯ Deliverables

#### âœ… YOLOv8 Processing
- **Script:** `scripts/enrich_with_yolo.py` uses `yolov8n.pt` to classify images (e.g., pills, creams, syringes).
- **Label Mapping:** Maps COCO classes to medical categories.
- **Logging:** Outputs to `scripts/logs/yolo_enrichment.log`.

---

#### âœ… Storage
- **Table:** `raw.image_classifications` with:
  - `classification_id` (PRIMARY KEY)
  - `message_id` (FOREIGN KEY)
  - `image_file`
  - `object_class`
  - `confidence`
  - `load_timestamp`

---

#### âœ… dbt Integration
- **Model:** `stg_image_classifications.sql` stages classifications with `confidence >= 0.5`.
- **Fact Table:** `fct_messages.sql` excludes `object_class`, joined at query time.
- **Analysis:** `analyze_object_detections.sql` enables object detection counts.

---

#### âœ… Testing
- **Tests:** `schema.yml` includes:
  - `unique` + `not_null` for `classification_id`
  - `relationships` for `message_id`
- **Custom Test:** `custom_confidence_range.sql` validates confidence scores.

---

#### âœ… Documentation
- **Docs:** dbt docs updated with new staging and marts.

---

### ðŸš€ Execution Instructions

```bash
# 1ï¸âƒ£ Install YOLOv8 dependencies
pip install ultralytics==8.3.15

# 2ï¸âƒ£ Process images
python scripts/enrich_with_yolo.py

# 3ï¸âƒ£ Verify enrichment
psql -U postgres -d telegram_medical -c "SELECT * FROM raw.image_classifications LIMIT 5;"
cat scripts/logs/yolo_enrichment.log

# 4ï¸âƒ£ Run dbt
make dbt-run
make dbt-test

# 5ï¸âƒ£ Generate & serve docs
make dbt-doc-generate
make dbt-doc-serve
```

ðŸ“ Notes
**fct_messages** optimized for one row per message, image classifications joined at query time.

YOLOv8 uses **yolov8n.pt**; can swap in a custom-trained medical model.

**analyze_object_detections.sql** supports insights like object counts per channel.

# ðŸ› ï¸ Task 4: Build an Analytical API with FastAPI

## âœ… Status: Completed

---

## ðŸŽ¯ Deliverables

### ðŸ“¦ FastAPI Application

- **api/main.py**: Defines endpoints.
- **api/database.py**: Manages PostgreSQL connections.
- **api/models.py**: Placeholder for future ORM (empty).
- **api/schemas.py**: Pydantic schemas for responses.
- **api/crud.py**: Query logic for endpoints.

### ðŸ”— Endpoints

- `GET /api/reports/top-products?limit=10`  
  âžœ Top products from text and image classifications.

- `GET /api/channels/{channel_name}/activity`  
  âžœ Posting activity with message and image counts.

- `GET /api/search/messages?query=paracetamol`  
  âžœ Messages matching a keyword.

- Logs are saved to **api/logs/api.log**.

### ðŸ³ Docker Integration

- **docker-compose.yml**: Added API service on port `8000`.

### ðŸ§ª Testing

- **api/tests/test_api.py**: Tests endpoints with `pytest` and `httpx`.

### ðŸ“š Documentation

- OpenAPI docs: [http://localhost:8000/docs](http://localhost:8000/docs)

- Updated `README.md`.

### ðŸ› ï¸ Fixes

- Corrected `dim_dates.date_day` to `date_id` in `crud.py` and `schemas.py` to align with `dim_dates.sql`.

---

## ðŸš€ Execution Instructions

```bash
# Create API log directory
mkdir -p api/logs

# Update dependencies
docker exec -it telegram_app pip install -r requirements.txt

# Save updated files:
# - api/schemas.py
# - api/crud.py
# - README.md

# Start services
docker-compose -f docker/docker-compose.yml up --build

# Test API with curl
curl http://localhost:8000/api/reports/top-products?limit=5
curl http://localhost:8000/api/channels/Chemed123/activity
curl http://localhost:8000/api/search/messages?query=paracetamol

# Or access OpenAPI docs
# http://localhost:8000/docs

# Run tests
docker exec -it telegram_api pytest api/tests/test_api.py

# Verify logs
cat api/logs/api.log
```
ðŸ“ Notes

Endpoints use query-time joins with **stg_image_classifications** for multi-object detection.

**top-products** combines text and image mentions for comprehensive product counts.

**channel_activity** uses **dim_dates.date_id** for accurate date grouping.

Tests verify response structure and status codes.

ðŸ”œ Next Steps
**Task 5**: Orchestrate pipeline with Dagster.

## ðŸ’¡ Key Learning Areas

> This repo demonstrates practical skills in:
> - Cloud-native ELT
> - dbt transformations & testing
> - Computer Vision integration (YOLOv8)
> - API design with FastAPI
> - Orchestration with Dagster
> - Secure, reproducible Docker workflows

---
